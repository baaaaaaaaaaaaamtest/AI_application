{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d494c3d4",
   "metadata": {},
   "source": [
    "# langGraph Practice 1\n",
    "\n",
    "\n",
    "\n",
    "![구성도](/home/ansgyqja/AI_application/images/9.structure_practice_1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8bcfcfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# API 키를 환경변수로 관리하기 위한 설정 파일\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# API 키 정보 로드\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "645b2c31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangSmith 추적을 시작합니다.\n",
      "[프로젝트명]\n",
      "langGraph_practice_1\n"
     ]
    }
   ],
   "source": [
    "# LangSmith 추적을 설정합니다. https://smith.langchain.com\n",
    "# !pip install -qU langchain-teddynote\n",
    "from langchain_teddynote import logging\n",
    "\n",
    "# 프로젝트 이름을 입력합니다.\n",
    "logging.langsmith(\"langGraph_practice_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "526519e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated,TypedDict,List,Literal\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langchain_teddynote.evaluator import GroundednessChecker\n",
    "from langchain import hub\n",
    "from operator import itemgetter\n",
    "from langchain_teddynote.messages import messages_to_history\n",
    "from langchain_teddynote.tools.tavily import TavilySearch\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "import sys, os\n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.dirname('get_naver_news'), '..')))\n",
    "from module.utils import re_write_prompt,get_gemini,format_docs\n",
    "from module.get_local_pdf import rag_pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9af94bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(TypedDict):\n",
    "    question: Annotated[list, add_messages]  # 질문(누적되는 list)\n",
    "    context : Annotated[str,'Context']\n",
    "    answer : Annotated[str,'Answer']\n",
    "    relevant : Annotated[str,'yes or no']\n",
    "    chat_history:Annotated[list,add_messages]\n",
    "\n",
    "# re write node\n",
    "def re_write_node(state : State) -> State:\n",
    "    chain = re_write_prompt | get_gemini() | StrOutputParser()\n",
    "    response = chain.invoke({\"question\": state['question']})\n",
    "    return State({\"question\":response})\n",
    "\n",
    "# retriever node\n",
    "def retriever_node(state : State) -> State:\n",
    "    retriever =rag_pdf().get_retriever()\n",
    "    latest_question = state[\"question\"][-1].content\n",
    "    document = retriever.invoke(latest_question)\n",
    "    context = format_docs(document)\n",
    "    return State({\"context\":context})\n",
    "\n",
    "def retriever_relevant(state : State):\n",
    "    question_answer_relevant = GroundednessChecker(\n",
    "        llm=get_gemini(), target=\"question-retrieval\"\n",
    "    ).create()\n",
    "    response = question_answer_relevant.invoke(\n",
    "        {\"question\": state[\"question\"][-1].content, \"context\": state[\"context\"]}\n",
    "    )\n",
    "    return State({\"relevant\":response.score})\n",
    "    \n",
    "\n",
    "def llm_answer_node(state: State):\n",
    "    prompt = hub.pull(\"teddynote/rag-prompt-chat-history\")\n",
    "    chain =(\n",
    "        {\n",
    "            \"question\": itemgetter(\"question\"),\n",
    "            \"context\": itemgetter(\"context\"),\n",
    "            \"chat_history\": itemgetter(\"chat_history\"),\n",
    "        } | \n",
    "        prompt | get_gemini() | StrOutputParser()\n",
    "        )\n",
    "    answer = chain.invoke(\n",
    "       {\n",
    "            \"question\": state['question'][-1].content,\n",
    "            \"context\": state['context'],\n",
    "            # \"chat_history\": \"\"\n",
    "            \"chat_history\": messages_to_history(state['chat_history']),  # 문자열 리스트로 변환\n",
    "        }\n",
    "    )\n",
    "    return {\n",
    "            \"answer\": answer,\n",
    "            \"chat_history\":[(\"user\" ,state['question'][-1].content),(\"assistant\" , answer)]\n",
    "            }\n",
    "\n",
    "def web_node(state:State):\n",
    "    web_search = TavilySearch()\n",
    "    search_context = web_search.search(query=state['question'])\n",
    "    return State({\"context\":search_context})\n",
    "\n",
    "def search_relevant(state : State):\n",
    "    retrieval_answer_relevant = GroundednessChecker(\n",
    "        llm=get_gemini(), target=\"retrieval-answer\"\n",
    "    ).create()\n",
    "    response = retrieval_answer_relevant.invoke(\n",
    "        {\"question\": state[\"question\"][-1].content, \"context\": state[\"context\"]}\n",
    "    )\n",
    "    return State({\"relevant\":response.score})\n",
    "   \n",
    "def is_retriever(state : State)-> Literal[\"llm_answer_node\",\"web_node\"]:\n",
    "    is_relevant = state.get(\"relevant\",\"no\")\n",
    "    if is_relevant == \"yes\":\n",
    "        return \"llm_answer_node\"\n",
    "    return \"web_node\"\n",
    "\n",
    "def is_search(state : State)-> Literal[\"llm_answer_node\",\"re_write_node\"]:\n",
    "    is_relevant = state.get(\"relevant\",\"no\")\n",
    "    if is_relevant == \"yes\":\n",
    "        return \"llm_answer_node\"\n",
    "    return \"re_write_node\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0011aca2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x7aba6002c650>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_graph = StateGraph(State)\n",
    "state_graph.add_node('re_write_node',re_write_node)\n",
    "state_graph.add_node('retriever_node',retriever_node)\n",
    "state_graph.add_node('retriever_relevant',retriever_relevant)\n",
    "state_graph.add_node('llm_answer_node',llm_answer_node)\n",
    "state_graph.add_node('web_node',web_node)\n",
    "state_graph.add_node('search_relevant',search_relevant)\n",
    "\n",
    "\n",
    "state_graph.add_edge(START,'re_write_node')\n",
    "state_graph.add_edge('re_write_node','retriever_node')\n",
    "state_graph.add_edge('retriever_node','retriever_relevant')\n",
    "state_graph.add_conditional_edges(\n",
    "    source='retriever_relevant',\n",
    "    path=is_retriever,\n",
    "    path_map={\n",
    "        \"llm_answer_node\": \"llm_answer_node\",  # 관련성이 있으면 답변을 생성합니다.\n",
    "        \"web_node\": \"web_node\",  # 관련성이 없으면 다시 검색합니다.\n",
    "    },\n",
    ")\n",
    "state_graph.add_edge('web_node','search_relevant')\n",
    "state_graph.add_conditional_edges(\n",
    "    source='search_relevant',\n",
    "    path=is_search,\n",
    "    path_map={\n",
    "        \"llm_answer_node\": \"llm_answer_node\",  # 관련성이 있으면 답변을 생성합니다.\n",
    "        \"re_write_node\": \"re_write_node\",  # 관련성이 없으면 다시 검색합니다.\n",
    "    },\n",
    ")\n",
    "state_graph.add_edge('llm_answer_node',END)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b803bc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "config:\n",
      "  flowchart:\n",
      "    curve: linear\n",
      "---\n",
      "graph TD;\n",
      "\t__start__([<p>__start__</p>]):::first\n",
      "\tre_write_node(re_write_node)\n",
      "\tretriever_node(retriever_node)\n",
      "\tretriever_relevant(retriever_relevant)\n",
      "\tllm_answer_node(llm_answer_node)\n",
      "\tweb_node(web_node)\n",
      "\tsearch_relevant(search_relevant)\n",
      "\t__end__([<p>__end__</p>]):::last\n",
      "\t__start__ --> re_write_node;\n",
      "\tre_write_node --> retriever_node;\n",
      "\tretriever_node --> retriever_relevant;\n",
      "\tretriever_relevant -.-> llm_answer_node;\n",
      "\tretriever_relevant -.-> web_node;\n",
      "\tsearch_relevant -.-> llm_answer_node;\n",
      "\tsearch_relevant -.-> re_write_node;\n",
      "\tweb_node --> search_relevant;\n",
      "\tllm_answer_node --> __end__;\n",
      "\tclassDef default fill:#f2f0ff,line-height:1.2\n",
      "\tclassDef first fill-opacity:0\n",
      "\tclassDef last fill:#bfb6fc\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 메모리 저장소 생성\n",
    "memory = MemorySaver()\n",
    "graph = state_graph.compile(checkpointer=memory)\n",
    "mermaid_code = graph.get_graph().draw_mermaid()\n",
    "print(mermaid_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a693768f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36mre_write_node\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "엔트로피의 투자 금액은 얼마인가요?\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36mretriever_relevant\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36mllm_answer_node\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "구글은 앤스로픽에 최대 20억 달러를 투자하기로 합의했으며, 이 중 5억 달러를 우선 투자하고 향후 15억 달러를 추가로 투자할 방침입니다.\n",
      "- ../data/SPRI_AI_Brief_2023년12월호_F.pdf (14쪽)"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableConfig\n",
    "from langchain_teddynote.messages import stream_graph, invoke_graph, random_uuid\n",
    "\n",
    "# config 설정(재귀 최대 횟수, thread_id)\n",
    "config = RunnableConfig(recursion_limit=10, configurable={\"thread_id\": random_uuid()})\n",
    "\n",
    "# 질문 입력\n",
    "inputs = State({'question':'엔트로피 투자금액'})\n",
    "\n",
    "# for event in graph.stream({'question':'엔트로피 투자금액'},config=config,stream_mode=\"values\"):\n",
    "#     print(event)\n",
    "# 그래프 실행\n",
    "stream_graph(graph, inputs, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c1853eaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': [HumanMessage(content='엔트로피 투자금액', additional_kwargs={}, response_metadata={}, id='60e0ac60-be6b-4449-8b90-9fd1357ce7f6'), HumanMessage(content='엔트로피의 투자 금액은 얼마인가요?', additional_kwargs={}, response_metadata={}, id='e80114b3-6fb4-44a2-9546-05e9f3523f67')], 'context': '<document><content>1. 정책/법제  2. 기업/산업 3. 기술/연구  4. 인력/교육\\n미국 프런티어 모델 포럼, 1,000 만 달러 규모의 AI 안전 기금 조성\\nn구글, 앤스로픽 , 마이크로소프트 , 오픈AI가 참여하는 프런티어 모델 포럼이 자선단체와 함께 AI \\n안전 연구를 위한 1,000 만 달러 규모의 AI 안전 기금을 조성\\nn프런티어 모델 포럼은 AI 모델의 취약점을 발견하고 검증하는 레드팀 활동을 지원하기 위한 \\n모델 평가 기법 개발에 자금을 중점 지원할 계획KEY Contents\\n£프런티어 모델 포럼, 자선단체와 함께 AI 안전 연구를 위한 기금 조성\\nn구글, 앤스로픽 , 마이크로소프트 , 오픈AI가 출범한 프런티어 모델 포럼이 2023년 10월 25일 AI 안전 \\n연구를 위한 기금을 조성한다고 발표\\n∙참여사들은 맥거번 재단(Patrick J. McGovern Foundation), 데이비드 앤 루실 패커드 재단(The \\nDavid and Lucile Packard Foundation ) 등의 자선단체와 함께 AI 안전 연구를 위한 기금에 \\n1,000 만 달러 이상을 기부 \\n∙또한 신기술의 거버넌스와 안전 분야에서 전문성을 갖춘 브루킹스 연구소 출신의 크리스 메서롤 (Chris \\nMeserole) 을 포럼의 상무이사로 임명\\nn최근 AI 기술이 급속히 발전하면서 AI 안전에 관한 연구가 부족한 시점에 , 포럼은 이러한 격차를 해소\\n하기 위해 AI 안전 기금을 조성\\n∙참여사들은 지난 7월 백악관 주재의 AI 안전 서약에서 외부자의 AI 시스템 취약점 발견과 신고를 \\n촉진하기로 약속했으며 , 약속을 이행하기 위해 기금을 활용해 외부 연구집단의 AI 시스템 평가에 \\n자금을 지원할 계획\\n£AI 안전 기금으로 AI 레드팀을 위한 모델 평가 기법 개발을 중점 지원할 계획\\nn프런티어 모델 포럼은 AI 안전 기금을 통해 AI 레드팀 활동을 위한 새로운 모델 평가 기법의  개발을 \\n중점 지원할 예정</content><source>../data/SPRI_AI_Brief_2023년12월호_F.pdf</source><page>10</page></document>\\n<document><content>18영국 옥스퍼드 인터넷 연구소 , AI 기술자의 임금이 평균 21% 높아\\nn옥스퍼드 인터넷 연구소의 연구에 따르면 특정 기술의 경제적 가치는 다른 기술과 결합 \\n가능성이 높을수록 높게 평가됨 \\nnAI의 확산은 기술의 경제적 가치에 크게 영향을 미치며 , AI 기술을 가진 근로자는 평균 21%, \\n최대 40% 높은 임금을 받을 수 있음  KEY Contents\\n£AI 기술 중 머신러닝 , 텐서플로우 , 딥러닝의 임금 프리미엄이 높게 평가\\nn옥스퍼드 인터넷 연구소 (Oxford Internet Institute) 가 2023년 10월 24일 962개 기술과 2만 5천 \\n명을 대상으로 한 연구에서 AI를 포함한 주요 기술의 경제적 가치를 분석한 결과를 발표 \\n∙연구에 따르면 한 기술의 경제적 가치는 근로자의 여타 역량과 얼마나 잘 결합하는지를 보여주는 \\n‘상보성 (complementarity)’ 에 따라 결정됨\\n∙특정 기술은 다른 기술과 결합 가능성이 높을수록 경제적 가치가 높아지며 , 일례로 데이터 분석과 같은 \\n기술은 여타 고부가가치 기술과 결합할 수 있어 가치가 높지만 , 사진 리터칭 같은 기술은 특정 기술과만 \\n결합할 수 있어 가치가 낮게 평가됨 \\n∙대부분 직업은 여러 기술의 조합이 필요하며 , 근로자의 재교육에서 경제적 효율성을 높이려면 기존 기술과 \\n신기술 간 상보성을 극대화할 필요\\nnAI의 확산은 기술의 경제적 가치에 크게 영향을 미치는 요소로 , AI 기술을 가진 근로자는 평균적으로  \\n21% 높은 임금을 획득 가능\\n∙AI 기술 중 근로자에 대한 경제적 가치(시간당 임금 증가율 기준) 측면에서 상위 5개 기술은 \\n머신러닝 (+40%), 텐서플로우 (+38%), 딥러닝 (+27%), 자연어처리 (+19%), 데이터 과학(+17%) 순</content><source>../data/SPRI_AI_Brief_2023년12월호_F.pdf</source><page>21</page></document>\\n<document><content>n에이전트로 인해 주목할 만한 변화는 고비용 서비스의 대중화로 특히 △의료 △교육 △생산성 △\\n엔터테인먼트 ·쇼핑의 4개 영역에서 대규모 변화 예상\\n∙(의료) 에이전트가 환자 분류를 지원하고 건강 문제에 대한 조언을 제공하며 치료의 필요 여부를 결정하면서 \\n의료진의 의사결정과 생산성 향상에 기여\\n∙(교육) 에이전트가 1대 1 가정교사의 역할을 맡아 모든 학생에게 평등한 교육 기회를 제공할 수 있으며 , \\n아이가 좋아하는 게임이나 노래 등을 활용해 시청각 기반의 풍부한 맞춤형 교육 경험을 제공\\n∙(생산성 ) 사용자의 아이디어를 기반으로 에이전트가 사업계획과 발표 자료 작성, 제품 이미지 생성을 \\n지원하며 , 임원의 개인 비서와 같은 역할도 수행 \\n∙(엔터테인먼트 ·쇼핑) 쇼핑 시 에이전트가 모든 리뷰를 읽고 요약해 최적의 제품을 추천하고 사용자 대신 \\n주문할 수 있으며 사용자의 관심사에 맞춤화된 뉴스와 엔터테인먼트를 구독 가능\\n☞ 출처 : GatesNotes, AI is about to completely change how you use computers, 2023.11.09.</content><source>../data/SPRI_AI_Brief_2023년12월호_F.pdf</source><page>16</page></document>\\n<document><content>1. 정책/법제  2. 기업/산업 3. 기술/연구  4. 인력/교육\\n구글, 앤스로픽에 20억 달러 투자로 생성 AI 협력 강화 \\nn구글이 앤스로픽에 최대 20억 달러 투자에 합의하고 5억 달러를 우선 투자했으며 , 앤스로픽은 \\n구글과 클라우드 서비스 사용 계약도 체결\\nn3대 클라우드 사업자인 구글, 마이크로소프트 , 아마존은 차세대 AI 모델의 대표 기업인 \\n앤스로픽 및 오픈AI와 협력을 확대하는 추세KEY Contents\\n£구글, 앤스로픽에 최대 20억 달러 투자 합의 및 클라우드 서비스 제공\\nn구글이 2023 년 10월 27일 앤스로픽에 최대 20억 달러를 투자하기로 합의했으며 , 이 중 5억 \\n달러를 우선 투자하고 향후 15억 달러를 추가로 투자할 방침\\n∙구글은 2023 년 2월 앤스로픽에 이미 5억 5,000 만 달러를 투자한 바 있으며 , 아마존도 지난 9월 \\n앤스로픽에 최대 40억 달러의 투자 계획을 공개\\n∙한편, 2023 년 11월 8일 블룸버그 보도에 따르면 앤스로픽은 구글의 클라우드 서비스 사용을 위해 \\n4년간 30억 달러 규모의 계약을 체결\\n∙오픈AI 창업자 그룹의 일원이었던 다리오 (Dario Amodei) 와 다니엘라 아모데이 (Daniela Amodei) \\n남매가 2021 년 설립한 앤스로픽은 챗GPT의 대항마 ‘클로드 (Claude)’ LLM을 개발\\nn아마존과 구글의 앤스로픽 투자에 앞서, 마이크로소프트는 차세대 AI 모델의 대표 주자인  오픈\\nAI와 협력을 확대\\n∙마이크로소프트는 오픈AI에 앞서 투자한 30억 달러에 더해 2023 년 1월 추가로 100억 달러를 \\n투자하기로 하면서 오픈AI의 지분 49%를 확보했으며 , 오픈AI는 마이크로소프트의 애저(Azure) \\n클라우드 플랫폼을 사용해 AI 모델을 훈련\\n£구글, 클라우드 경쟁력 강화를 위해 생성 AI 투자 확대\\nn구글은 수익률이 높은 클라우드 컴퓨팅 시장에서 아마존과 마이크로소프트를 따라잡고자 생성 AI를 \\n통한 기업 고객의 클라우드 지출 확대를 위해 AI 투자를 지속</content><source>../data/SPRI_AI_Brief_2023년12월호_F.pdf</source><page>14</page></document>', 'answer': '구글은 앤스로픽에 최대 20억 달러를 투자하기로 합의했으며, 이 중 5억 달러를 우선 투자하고 향후 15억 달러를 추가로 투자할 방침입니다.\\n- ../data/SPRI_AI_Brief_2023년12월호_F.pdf (14쪽)', 'relevant': 'yes', 'chat_history': [HumanMessage(content='엔트로피의 투자 금액은 얼마인가요?', additional_kwargs={}, response_metadata={}, id='63c4fbe1-399d-4dce-b685-f7ca4e941e90'), AIMessage(content='구글은 앤스로픽에 최대 20억 달러를 투자하기로 합의했으며, 이 중 5억 달러를 우선 투자하고 향후 15억 달러를 추가로 투자할 방침입니다.\\n- ../data/SPRI_AI_Brief_2023년12월호_F.pdf (14쪽)', additional_kwargs={}, response_metadata={}, id='6c090122-926f-49c4-b41d-121081f272a2')]}\n"
     ]
    }
   ],
   "source": [
    "# 최종 출력\n",
    "\n",
    "outputs = graph.get_state(config).values\n",
    "print(outputs)\n",
    "# print(f'Original Question: {outputs[\"question\"][0].content}')\n",
    "# print(f'Rewritten Question: {outputs[\"question\"][-1].content}')\n",
    "# print(\"===\" * 20)\n",
    "# print(f'Answer:\\n{outputs[\"answer\"]}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-kr-EGILWU2T-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
