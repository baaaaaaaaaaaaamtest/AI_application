{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d494c3d4",
   "metadata": {},
   "source": [
    "# langGraph Practice 1\n",
    "\n",
    "\n",
    "\n",
    "![êµ¬ì„±ë„](/home/ansgyqja/AI_application/images/9.structure_practice_1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8bcfcfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# API í‚¤ë¥¼ í™˜ê²½ë³€ìˆ˜ë¡œ ê´€ë¦¬í•˜ê¸° ìœ„í•œ ì„¤ì • íŒŒì¼\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# API í‚¤ ì •ë³´ ë¡œë“œ\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "645b2c31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangSmith ì¶”ì ì„ ì‹œì‘í•©ë‹ˆë‹¤.\n",
      "[í”„ë¡œì íŠ¸ëª…]\n",
      "langGraph_practice_1\n"
     ]
    }
   ],
   "source": [
    "# LangSmith ì¶”ì ì„ ì„¤ì •í•©ë‹ˆë‹¤. https://smith.langchain.com\n",
    "# !pip install -qU langchain-teddynote\n",
    "from langchain_teddynote import logging\n",
    "\n",
    "# í”„ë¡œì íŠ¸ ì´ë¦„ì„ ì…ë ¥í•©ë‹ˆë‹¤.\n",
    "logging.langsmith(\"langGraph_practice_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "526519e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated,TypedDict,List,Literal\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langchain_teddynote.evaluator import GroundednessChecker\n",
    "from langchain import hub\n",
    "from operator import itemgetter\n",
    "from langchain_teddynote.messages import messages_to_history\n",
    "from langchain_teddynote.tools.tavily import TavilySearch\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "import sys, os\n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.dirname('get_naver_news'), '..')))\n",
    "from module.utils import re_write_prompt,get_gemini,format_docs\n",
    "from module.get_local_pdf import rag_pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9af94bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(TypedDict):\n",
    "    question: Annotated[list, add_messages]  # ì§ˆë¬¸(ëˆ„ì ë˜ëŠ” list)\n",
    "    context : Annotated[str,'Context']\n",
    "    answer : Annotated[str,'Answer']\n",
    "    relevant : Annotated[str,'yes or no']\n",
    "    chat_history:Annotated[list,add_messages]\n",
    "\n",
    "# re write node\n",
    "def re_write_node(state : State) -> State:\n",
    "    chain = re_write_prompt | get_gemini() | StrOutputParser()\n",
    "    response = chain.invoke({\"question\": state['question']})\n",
    "    return State({\"question\":response})\n",
    "\n",
    "# retriever node\n",
    "def retriever_node(state : State) -> State:\n",
    "    retriever =rag_pdf().get_retriever()\n",
    "    latest_question = state[\"question\"][-1].content\n",
    "    document = retriever.invoke(latest_question)\n",
    "    context = format_docs(document)\n",
    "    return State({\"context\":context})\n",
    "\n",
    "def retriever_relevant(state : State):\n",
    "    question_answer_relevant = GroundednessChecker(\n",
    "        llm=get_gemini(), target=\"question-retrieval\"\n",
    "    ).create()\n",
    "    response = question_answer_relevant.invoke(\n",
    "        {\"question\": state[\"question\"][-1].content, \"context\": state[\"context\"]}\n",
    "    )\n",
    "    return State({\"relevant\":response.score})\n",
    "    \n",
    "\n",
    "def llm_answer_node(state: State):\n",
    "    prompt = hub.pull(\"teddynote/rag-prompt-chat-history\")\n",
    "    chain =(\n",
    "        {\n",
    "            \"question\": itemgetter(\"question\"),\n",
    "            \"context\": itemgetter(\"context\"),\n",
    "            \"chat_history\": itemgetter(\"chat_history\"),\n",
    "        } | \n",
    "        prompt | get_gemini() | StrOutputParser()\n",
    "        )\n",
    "    answer = chain.invoke(\n",
    "       {\n",
    "            \"question\": state['question'][-1].content,\n",
    "            \"context\": state['context'],\n",
    "            # \"chat_history\": \"\"\n",
    "            \"chat_history\": messages_to_history(state['chat_history']),  # ë¬¸ìì—´ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜\n",
    "        }\n",
    "    )\n",
    "    return {\n",
    "            \"answer\": answer,\n",
    "            \"chat_history\":[(\"user\" ,state['question'][-1].content),(\"assistant\" , answer)]\n",
    "            }\n",
    "\n",
    "def web_node(state:State):\n",
    "    web_search = TavilySearch()\n",
    "    search_context = web_search.search(query=state['question'][-1].content)\n",
    "    return State({\"context\":search_context})\n",
    "\n",
    "def search_relevant(state : State):\n",
    "    question_answer_relevant = GroundednessChecker(\n",
    "        llm=get_gemini(), target=\"question-retrieval\"\n",
    "    ).create()\n",
    "    response = question_answer_relevant.invoke(\n",
    "        {\"question\": state[\"question\"][-1].content, \"context\": state[\"context\"]}\n",
    "    )\n",
    "    return State({\"relevant\":response.score})\n",
    "   \n",
    "def is_retriever(state : State)-> Literal[\"llm_answer_node\",\"web_node\"]:\n",
    "    is_relevant = state.get(\"relevant\",\"no\")\n",
    "    if is_relevant == \"yes\":\n",
    "        return \"llm_answer_node\"\n",
    "    return \"web_node\"\n",
    "\n",
    "def is_search(state : State)-> Literal[\"llm_answer_node\",\"re_write_node\"]:\n",
    "    is_relevant = state.get(\"relevant\",\"no\")\n",
    "    if is_relevant == \"yes\":\n",
    "        return \"llm_answer_node\"\n",
    "    return \"re_write_node\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0011aca2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x751b08ed6650>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_graph = StateGraph(State)\n",
    "state_graph.add_node('re_write_node',re_write_node)\n",
    "state_graph.add_node('retriever_node',retriever_node)\n",
    "state_graph.add_node('retriever_relevant',retriever_relevant)\n",
    "state_graph.add_node('llm_answer_node',llm_answer_node)\n",
    "state_graph.add_node('web_node',web_node)\n",
    "state_graph.add_node('search_relevant',search_relevant)\n",
    "\n",
    "\n",
    "state_graph.add_edge(START,'re_write_node')\n",
    "state_graph.add_edge('re_write_node','retriever_node')\n",
    "state_graph.add_edge('retriever_node','retriever_relevant')\n",
    "state_graph.add_conditional_edges(\n",
    "    source='retriever_relevant',\n",
    "    path=is_retriever,\n",
    "    path_map={\n",
    "        \"llm_answer_node\": \"llm_answer_node\",  # ê´€ë ¨ì„±ì´ ìˆìœ¼ë©´ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "        \"web_node\": \"web_node\",  # ê´€ë ¨ì„±ì´ ì—†ìœ¼ë©´ ë‹¤ì‹œ ê²€ìƒ‰í•©ë‹ˆë‹¤.\n",
    "    },\n",
    ")\n",
    "state_graph.add_edge('web_node','search_relevant')\n",
    "state_graph.add_conditional_edges(\n",
    "    source='search_relevant',\n",
    "    path=is_search,\n",
    "    path_map={\n",
    "        \"llm_answer_node\": \"llm_answer_node\",  # ê´€ë ¨ì„±ì´ ìˆìœ¼ë©´ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "        \"re_write_node\": \"re_write_node\",  # ê´€ë ¨ì„±ì´ ì—†ìœ¼ë©´ ë‹¤ì‹œ ê²€ìƒ‰í•©ë‹ˆë‹¤.\n",
    "    },\n",
    ")\n",
    "state_graph.add_edge('llm_answer_node',END)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b803bc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "config:\n",
      "  flowchart:\n",
      "    curve: linear\n",
      "---\n",
      "graph TD;\n",
      "\t__start__([<p>__start__</p>]):::first\n",
      "\tre_write_node(re_write_node)\n",
      "\tretriever_node(retriever_node)\n",
      "\tretriever_relevant(retriever_relevant)\n",
      "\tllm_answer_node(llm_answer_node)\n",
      "\tweb_node(web_node)\n",
      "\tsearch_relevant(search_relevant)\n",
      "\t__end__([<p>__end__</p>]):::last\n",
      "\t__start__ --> re_write_node;\n",
      "\tre_write_node --> retriever_node;\n",
      "\tretriever_node --> retriever_relevant;\n",
      "\tretriever_relevant -.-> llm_answer_node;\n",
      "\tretriever_relevant -.-> web_node;\n",
      "\tsearch_relevant -.-> llm_answer_node;\n",
      "\tsearch_relevant -.-> re_write_node;\n",
      "\tweb_node --> search_relevant;\n",
      "\tllm_answer_node --> __end__;\n",
      "\tclassDef default fill:#f2f0ff,line-height:1.2\n",
      "\tclassDef first fill-opacity:0\n",
      "\tclassDef last fill:#bfb6fc\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ë©”ëª¨ë¦¬ ì €ì¥ì†Œ ìƒì„±\n",
    "memory = MemorySaver()\n",
    "graph = state_graph.compile(checkpointer=memory)\n",
    "mermaid_code = graph.get_graph().draw_mermaid()\n",
    "print(mermaid_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a693768f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36mre_write_node\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "ì˜¤ëŠ˜ì˜ ë‚ ì”¨ ì •ë³´\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36mretriever_relevant\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36msearch_relevant\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36mllm_answer_node\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "ì˜¤ëŠ˜(16ì¼)ì€ ì „êµ­ ëŒ€ì²´ë¡œ íë¦¬ê³  ëŠ¦ì€ ì˜¤í›„ê¹Œì§€ ì†Œë‚˜ê¸°ê°€ ë‚´ë¦¬ê² ìœ¼ë©°, ë°¤ë¶€í„° ì¤‘ë¶€ì§€ë°©(ê°•ì›ë™í•´ì•ˆ ì œì™¸)ê³¼ ì „ë¶ë¶ë¶€ì—ëŠ” ë¹„ê°€ ì˜¤ê² ìŠµë‹ˆë‹¤. ë‚´ì¼(17ì¼)ì€ ì „êµ­ì´ ëŒ€ì²´ë¡œ íë¦¬ê³  ìƒˆë²½ë¶€í„° ì¤‘ë¶€ì§€ë°©(ê°•ì›ë™í•´ì•ˆ ì œì™¸)ê³¼ ì „ë¼ê¶Œì— ë¹„ê°€ ì‹œì‘ë˜ì–´ ì˜¤ì „ë¶€í„° ê°•ì›ë™í•´ì•ˆê³¼ ê²½ìƒê¶Œ, ì œì£¼ë„ì—ë„ ë¹„ê°€ ì˜¤ê² ìœ¼ë©° ë°¤ì— ì°¨ì°¨ ê·¸ì¹˜ê² ìŠµë‹ˆë‹¤. ëª¨ë ˆ(18ì¼)ëŠ” ì „êµ­ì´ ê°€ë” êµ¬ë¦„ ë§ê² ìœ¼ë‚˜ ê°•ì›ì˜ë™ì€ ëŒ€ì²´ë¡œ íë¦¬ê³  ì˜¤ì „ê¹Œì§€ ê°•ì›ì˜ë™, ê²½ìƒê¶Œ, ì œì£¼ë„ì— ë¹„ê°€ ì˜¤ê² ìŠµë‹ˆë‹¤. ê¸€í”¼(19ì¼)ëŠ” ì „êµ­ì´ ëŒ€ì²´ë¡œ íë¦¬ê³  ìƒˆë²½ë¶€í„° ì œì£¼ë„, ì˜¤í›„ë¶€í„° ì¤‘ë¶€ì§€ë°©(ê°•ì›ì˜ë™ ì œì™¸)ê³¼ ì „ë¼ì„œí•´ì•ˆ, ê²½ë¶ë¶ë¶€, ë°¤ë¶€í„° ê·¸ ë°–ì˜ ì „ë¼ê¶Œê³¼ ê²½ìƒì„œë¶€ì— ë¹„ê°€ ì˜¤ê² ìŠµë‹ˆë‹¤.\n",
      "\n",
      "- https://www.weather.go.kr/weather/forecast/timeseries.jsp"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableConfig\n",
    "from langchain_teddynote.messages import stream_graph, invoke_graph, random_uuid\n",
    "\n",
    "\n",
    "question = 'ì˜¤ëŠ˜ì˜ ë‚ ì”¨'\n",
    "# ì§ˆë¬¸ ì…ë ¥\n",
    "inputs = State({'question':question})\n",
    "\n",
    "# for event in graph.stream({'question':'ì—”íŠ¸ë¡œí”¼ íˆ¬ìê¸ˆì•¡'},config=config,stream_mode=\"values\"):\n",
    "#     print(event)\n",
    "# ê·¸ë˜í”„ ì‹¤í–‰\n",
    "stream_graph(graph, inputs, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1853eaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Question: ì˜¤ëŠ˜ì˜ ë‚ ì”¨\n",
      "Rewritten Question: ì˜¤ëŠ˜ì˜ ë‚ ì”¨ ì •ë³´\n",
      "============================================================\n",
      "Answer:\n",
      "ì˜¤ëŠ˜(16ì¼)ì€ ì „êµ­ ëŒ€ì²´ë¡œ íë¦¬ê³  ëŠ¦ì€ ì˜¤í›„ê¹Œì§€ ì†Œë‚˜ê¸°ê°€ ë‚´ë¦¬ê² ìœ¼ë©°, ë°¤ë¶€í„° ì¤‘ë¶€ì§€ë°©(ê°•ì›ë™í•´ì•ˆ ì œì™¸)ê³¼ ì „ë¶ë¶ë¶€ì—ëŠ” ë¹„ê°€ ì˜¤ê² ìŠµë‹ˆë‹¤. ë‚´ì¼(17ì¼)ì€ ì „êµ­ì´ ëŒ€ì²´ë¡œ íë¦¬ê³  ìƒˆë²½ë¶€í„° ì¤‘ë¶€ì§€ë°©(ê°•ì›ë™í•´ì•ˆ ì œì™¸)ê³¼ ì „ë¼ê¶Œì— ë¹„ê°€ ì‹œì‘ë˜ì–´ ì˜¤ì „ë¶€í„° ê°•ì›ë™í•´ì•ˆê³¼ ê²½ìƒê¶Œ, ì œì£¼ë„ì—ë„ ë¹„ê°€ ì˜¤ê² ìœ¼ë©° ë°¤ì— ì°¨ì°¨ ê·¸ì¹˜ê² ìŠµë‹ˆë‹¤. ëª¨ë ˆ(18ì¼)ëŠ” ì „êµ­ì´ ê°€ë” êµ¬ë¦„ ë§ê² ìœ¼ë‚˜ ê°•ì›ì˜ë™ì€ ëŒ€ì²´ë¡œ íë¦¬ê³  ì˜¤ì „ê¹Œì§€ ê°•ì›ì˜ë™, ê²½ìƒê¶Œ, ì œì£¼ë„ì— ë¹„ê°€ ì˜¤ê² ìŠµë‹ˆë‹¤. ê¸€í”¼(19ì¼)ëŠ” ì „êµ­ì´ ëŒ€ì²´ë¡œ íë¦¬ê³  ìƒˆë²½ë¶€í„° ì œì£¼ë„, ì˜¤í›„ë¶€í„° ì¤‘ë¶€ì§€ë°©(ê°•ì›ì˜ë™ ì œì™¸)ê³¼ ì „ë¼ì„œí•´ì•ˆ, ê²½ë¶ë¶ë¶€, ë°¤ë¶€í„° ê·¸ ë°–ì˜ ì „ë¼ê¶Œê³¼ ê²½ìƒì„œë¶€ì— ë¹„ê°€ ì˜¤ê² ìŠµë‹ˆë‹¤.\n",
      "\n",
      "- https://www.weather.go.kr/weather/forecast/timeseries.jsp\n"
     ]
    }
   ],
   "source": [
    "# ìµœì¢… ì¶œë ¥\n",
    "\n",
    "outputs = graph.get_state(config).values\n",
    "# print(outputs)\n",
    "print(f'Original Question: {outputs[\"question\"][0].content}')\n",
    "print(f'Rewritten Question: {outputs[\"question\"][-1].content}')\n",
    "print(\"===\" * 20)\n",
    "print(f'Answer:\\n{outputs[\"answer\"]}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-kr-EGILWU2T-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
