{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cce76cb2",
   "metadata": {},
   "source": [
    "# 기본 구조 작성하기\n",
    "\n",
    "### 아래의 그림 구조 만들어보기\n",
    "\n",
    "1. 사용자 요청\n",
    "2. 쿼리 3가지로 증강\n",
    "3. gpt 단독 요청\n",
    "4. 결과 relevant\n",
    "5. rewrite query and 2번 반복\n",
    "6. 종료\n",
    "\n",
    "![\"structure\"](/home/ansgyqja/AI_application/images/relevant.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa7a132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# API 키를 환경변수로 관리하기 위한 설정 파일\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# API 키 정보 로드\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a44441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangSmith 추적을 설정합니다. https://smith.langchain.com\n",
    "# !pip install -qU langchain-teddynote\n",
    "from langchain_teddynote import logging\n",
    "\n",
    "# 프로젝트 이름을 입력합니다.\n",
    "logging.langsmith(\"CH15-Agentic-RAG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5992999",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.dirname('get_naver_news'), '..')))\n",
    "from module.get_mcp_client import get_mcp_client\n",
    "from module.get_news_origin import get_news_origin\n",
    "from module.get_local_pdf import rag_pdf\n",
    "from module.utils import convert_docs_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c86dad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated,Literal,List\n",
    "from langchain_core.documents import Document\n",
    "from typing_extensions import TypedDict\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_teddynote.tools.tavily import TavilySearch\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langchain_core.messages import SystemMessage, RemoveMessage, HumanMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from operator import itemgetter\n",
    "from langchain import hub\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_teddynote.messages import messages_to_history\n",
    "from langchain_teddynote.evaluator import GroundednessChecker\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ccadf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = hub.pull(\"teddynote/rag-prompt-chat-history\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d562a00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# State 정의\n",
    "class State(TypedDict):\n",
    "    context: Annotated[str, 'Context']  # retrieve 검색 결과\n",
    "    answer: Annotated[str, 'Answer'] # 사용자에게 제공하는 답변\n",
    "    question: Annotated[str, \"Question\"] # 사용자 질의\n",
    "    binary_score: Annotated[str, \"Relevance\"] # relevance 결과 yes or no\n",
    "    chat_history:Annotated[list,add_messages]\n",
    "\n",
    "def retrieve(state: State) -> State:\n",
    "    # retrieve: 검색\n",
    "    question = state['question']\n",
    "    documents = rag_pdf().search_by_keyword(question)\n",
    "    context = convert_docs_str(documents)\n",
    "    return State({\"context\": context})\n",
    "    \n",
    "def gpt_generate(state: State) -> State:\n",
    "    # gpt_request: 검색\n",
    "    llm= ChatOpenAI(model='gpt-4.1-mini',temperature=0.5)\n",
    "    chain = (\n",
    "        {\n",
    "            \"question\": itemgetter(\"question\"),\n",
    "            \"context\": itemgetter(\"context\"),\n",
    "            \"chat_history\": itemgetter(\"chat_history\"),\n",
    "\n",
    "        }\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    documents = chain.invoke(\n",
    "        {\n",
    "        \"question\": state['question'],\n",
    "        \"context\": state['context'],\n",
    "        \"chat_history\":messages_to_history(state['chat_history'])\n",
    "        }\n",
    "    )\n",
    "    return {\n",
    "            \"answer\": documents,\n",
    "            \"chat_history\":[(\"user\",state['question']),(\"assistant\" , documents)]\n",
    "            }\n",
    "\n",
    "def claude_generate(state: State) -> State:\n",
    "    # claude_request: 검색\n",
    "    documents = \"claude 답변\"\n",
    "    return {\"answer\": documents}\n",
    "\n",
    "def relevance_check(state:State) -> State:\n",
    "    question_retrieval_relevant = GroundednessChecker(\n",
    "        llm=ChatOpenAI(model=\"gpt-4o-mini\", temperature=0), target=\"question-retrieval\"\n",
    "    ).create()\n",
    "    relevant = question_retrieval_relevant.invoke({\"question\":state[\"question\"],\"context\":state[\"context\"]})\n",
    "    # gpt or claude response 검색\n",
    " \n",
    "    return State({'binary_score':relevant.score})\n",
    "\n",
    "def sum_answer(state:State)->State:\n",
    "    # sum_answer 검색\n",
    "    answer = \"종합된 답변\"\n",
    "    return State(answer=answer)\n",
    "\n",
    "def decision(state: State) -> Literal[\"rewrite_query\",END]:\n",
    "    # 의사결정\n",
    "    # 로직을 추가할 수 가 있고요.\n",
    "    \n",
    "    if state[\"binary_score\"] == \"no\" :\n",
    "        return \"rewrite_query\"\n",
    "    return END\n",
    "    \n",
    "def rewrite_query(state: State)->State:\n",
    "    question = \"새로운 질문\"\n",
    "    return State({'question':question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5b280d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "state_graph = StateGraph(State)\n",
    "\n",
    "state_graph.add_node('retrieve',retrieve)\n",
    "state_graph.add_node('gpt_generate',gpt_generate)\n",
    "state_graph.add_node('gpt_relevance_check',relevance_check)\n",
    "state_graph.add_node('rewrite_query',rewrite_query)\n",
    "\n",
    "\n",
    "\n",
    "# state_graph.add_node('claude_generate',claude_generate)\n",
    "# state_graph.add_node('claude_relevance_check',relevance_check)\n",
    "# state_graph.add_node('sum_answer',sum_answer)\n",
    "\n",
    "\n",
    "state_graph.add_edge(START,'retrieve')\n",
    "\n",
    "state_graph.add_edge('retrieve','gpt_generate')\n",
    "state_graph.add_edge('gpt_generate','gpt_relevance_check')\n",
    "state_graph.add_conditional_edges(\n",
    "    source='gpt_relevance_check',\n",
    "    path=decision,\n",
    ")\n",
    "state_graph.add_edge('rewrite_query','retrieve')\n",
    "state_graph.add_edge('gpt_generate',END)\n",
    "# state_graph.add_edge('gpt_generate','gpt_relevance_check')\n",
    "# state_graph.add_edge('gpt_relevance_check','sum_answer')\n",
    "\n",
    "# state_graph.add_edge('retrieve','claude_generate')\n",
    "# state_graph.add_edge('claude_generate','claude_relevance_check')\n",
    "# state_graph.add_edge('claude_relevance_check','sum_answer')\n",
    "\n",
    "# state_graph.add_conditional_edges(\n",
    "#     source='sum_answer',\n",
    "#     path=decision\n",
    "# )\n",
    "# state_graph.add_edge('rewrite_query','retrieve')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffdba61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 메모리 저장소 생성\n",
    "memory = MemorySaver()\n",
    "graph = state_graph.compile(checkpointer=memory)\n",
    "mermaid_code = graph.get_graph().draw_mermaid()\n",
    "print(mermaid_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c548e287",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableConfig\n",
    "\n",
    "config = RunnableConfig(\n",
    "    recursion_limit=10,  # 최대 10개의 노드까지 방문. 그 이상은 RecursionError 발생\n",
    "    configurable={\"thread_id\": \"00\"},  # 스레드 ID 설정\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1599d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "question =  {\"question\":\"오늘의 날씨\"}\n",
    "# for event in graph.stream(question,config=config,stream_mode=\"values\"):\n",
    "#     print(\"**\"*30)\n",
    "#     print(event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4231d025",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_teddynote.messages import invoke_graph, stream_graph, random_uuid\n",
    "\n",
    "# invoke_graph(graph,question,config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d0f04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "stream_graph(graph,question,config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-kr-EGILWU2T-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
