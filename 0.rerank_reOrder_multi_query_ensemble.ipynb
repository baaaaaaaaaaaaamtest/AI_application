{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb4cac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter \n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain import PromptTemplate\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import CrossEncoderReranker\n",
    "from langchain_community.cross_encoders import HuggingFaceCrossEncoder\n",
    "from langchain_community.document_transformers import LongContextReorder\n",
    "from operator import itemgetter\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
    "from langchain_ollama import OllamaLLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94c2b2e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# API 키를 환경변수로 관리하기 위한 설정 파일\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# API 키 정보 로드\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03ee0ae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangSmith 추적을 시작합니다.\n",
      "[프로젝트명]\n",
      "CH00-RAG-Chatbot\n"
     ]
    }
   ],
   "source": [
    "# LangSmith 추적을 설정합니다. https://smith.langchain.com\n",
    "# !pip install langchain-teddynote\n",
    "from langchain_teddynote import logging\n",
    "\n",
    "# 프로젝트 이름을 입력합니다.\n",
    "logging.langsmith(\"CH00-RAG-Chatbot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7edd4ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "OLLAMA_BASE_URL = \"http://localhost:11434\" # Ollama 서버 주소\n",
    "OLLAMA_CHAT_MODEL = \"ollama-ko-0710:latest\" # Ollama에서 실행중인 모델 이름"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df421d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_split_docs():\n",
    "    # 1. 텍스트 청크 분할 (chunk_size=1000, chunk_overlap=50)\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "\n",
    "    # 2. Text loader\n",
    "    loader = TextLoader(\"./data/text.txt\", encoding=\"utf-8\")\n",
    "    return loader.load_and_split(text_splitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e800fcc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_retriever(split_docs): \n",
    "    model_name = \"intfloat/multilingual-e5-large-instruct\"\n",
    "    hf_embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=model_name,\n",
    "        model_kwargs={\"device\": \"cuda\"},  # cuda, cpu\n",
    "        encode_kwargs={\"normalize_embeddings\": True},\n",
    "    )\n",
    "    db = FAISS.from_documents(documents=split_docs, embedding=hf_embeddings)\n",
    "    retriever = db.as_retriever(\n",
    "        search_type=\"mmr\", search_kwargs={\"k\": 10, \"lambda_mult\": 0.25, \"fetch_k\": 20}\n",
    "    )\n",
    "    return retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "968f87ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bm25_retriever(split_docs):\n",
    "    bm25_retriever = BM25Retriever.from_documents(\n",
    "        split_docs\n",
    "    )\n",
    "    bm25_retriever.k = 10  # BM25Retriever의 검색 결과 개수를 1로 설정합니다.\n",
    "    return bm25_retriever\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67508888",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reranker(retriever):\n",
    "    reranker = HuggingFaceCrossEncoder(model_name=\"BAAI/bge-reranker-v2-m3\")\n",
    "    compressor = CrossEncoderReranker(model=reranker, top_n=10)\n",
    "    return ContextualCompressionRetriever(\n",
    "        base_compressor=compressor, base_retriever=retriever\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "421ad485",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reorder_documents(docs):\n",
    "    # 재정렬\n",
    "    reordering = LongContextReorder()\n",
    "    reordered_docs = reordering.transform_documents(docs)\n",
    "    return reordered_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16ed2c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt():\n",
    "    template = \"\"\"당신은 유용한 AI 어시스턴트입니다. 사용자의 질의에 대해 친절하고 정확하게 답변해야 합니다.\n",
    "    You are a helpful AI assistant, you'll need to answer users' queries in a friendly and accurate manner.\n",
    "    모든 대답은 반드시 한국말로 대답해주세요.\n",
    "    \n",
    "    문서 내용:\n",
    "    {context}\n",
    "    질문: {question}\n",
    "    답변:\"\"\"\n",
    "    prompt = PromptTemplate(template=template, input_variables=[\"context\", \"question\"])\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "571ddc53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_multi_query_retriever(retriever):\n",
    "    # 다중 질의어 생성\n",
    "    llm = ChatOpenAI(temperature=0, model=\"gpt-4o-mini\")\n",
    "    multiquery_retriever = MultiQueryRetriever.from_llm(  # MultiQueryRetriever를 언어 모델을 사용하여 초기화합니다.\n",
    "    # 벡터 데이터베이스의 retriever와 언어 모델을 전달합니다.\n",
    "    retriever=retriever,\n",
    "    llm=llm,\n",
    "    )\n",
    "    return multiquery_retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c67131",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_esenmble_retriever(retriever1, retriever2):\n",
    "    ensemble_retriever = EnsembleRetriever(\n",
    "        retrievers=[retriever1, retriever2],\n",
    "        weights=[0.7, 0.5],  # 각 리트리버의 가중치를 설정합니다.\n",
    "        k=10  # 최종적으로 반환할 문서의 개수를 설정합니다.\n",
    "    )\n",
    "    return ensemble_retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a62a92",
   "metadata": {},
   "source": [
    "## ReRank+ ReOrder  추가 버전"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28d59f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def general_llm():\n",
    "\n",
    "    import logging\n",
    "    logging.basicConfig()\n",
    "    logging.getLogger(\"langchain.retrievers.multi_query\").setLevel(logging.INFO)\n",
    "\n",
    "    split_docs = get_split_docs()\n",
    "\n",
    "    faiss_retriever = get_retriever(split_docs)\n",
    "    \n",
    "    bm25_retriever = get_bm25_retriever(split_docs)\n",
    "\n",
    "    faiss_multi_retriever = get_multi_query_retriever(faiss_retriever)\n",
    "\n",
    "    bm25_multi_retriever = get_multi_query_retriever(bm25_retriever)\n",
    "\n",
    "    esenmble_retriever = get_esenmble_retriever(faiss_multi_retriever, bm25_multi_retriever)\n",
    "\n",
    "    compression_retriever = get_reranker(esenmble_retriever)\n",
    "    \n",
    "    prompt= get_prompt()\n",
    "    \n",
    "    \n",
    "    # 7. Ollama LLM 초기화\n",
    "    # llm = OllamaLLM(model=OLLAMA_CHAT_MODEL, base_url=OLLAMA_BASE_URL,temperature=1, max_tokens=1024)\n",
    "    llm = ChatOpenAI(model_name=\"gpt-4.1-mini\", temperature=0.7, max_tokens=1024)\n",
    "    chain = (\n",
    "    {\n",
    "        \"context\": itemgetter(\"question\")\n",
    "        | compression_retriever\n",
    "        | RunnableLambda(reorder_documents),  # 질문을 기반으로 문맥을 검색합니다.\n",
    "        \"question\": itemgetter(\"question\"),  # 질문을 추출합니다.\n",
    "    }\n",
    "    | prompt  # 프롬프트 템플릿에 값을 전달합니다.\n",
    "    | llm # 언어 모델에 프롬프트를 전달합니다.\n",
    "    | StrOutputParser()  # 모델의 출력을 문자열로 파싱합니다.\n",
    "    )\n",
    "\n",
    "    answer = chain.invoke({\"question\": \"삼성전자 하반기 실적 전망은?\"})\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25167a07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:langchain.retrievers.multi_query:Generated queries: ['삼성전자의 2023년 하반기 실적 예측은 어떻게 되나요?  ', '올해 하반기 삼성전자의 재무 성과에 대한 전망은 무엇인가요?  ', '삼성전자의 하반기 실적에 대한 분석이나 예측이 있나요?']\n",
      "INFO:langchain.retrievers.multi_query:Generated queries: ['삼성전자의 2023년 하반기 실적 예측은 어떻게 되나요?  ', '올해 하반기 삼성전자의 재무 성과에 대한 전망은 무엇인가요?  ', '삼성전자의 하반기 실적에 대한 분석이나 예측이 있나요?']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'삼성전자의 하반기 실적 전망은 긍정적입니다. NH투자증권 류영호 연구원은 파운드리 부문에서 테슬라와 애플 등 주요 고객사를 확보했으며, 10나노급 6세대(1c) 수율 개선과 하반기 엔비디아 고대역폭 메모리(HBM) 공급 기대감이 주가에 긍정적인 영향을 미치고 있다고 밝혔습니다. 또한 3분기 영업이익은 약 9조원으로, 전년 동기 대비 1.9% 감소하나 직전 분기 대비 91.9% 증가할 것으로 전망되고 있습니다.\\n\\n미래에셋증권은 올해와 내년 삼성전자 영업이익 전망치를 각각 2.9%, 5.3% 상향 조정했으며, 목표 주가도 8만8000원에서 9만6000원으로 9% 올렸습니다. 이는 내년 메모리 반도체 공급 부족 상황과 차세대 고대역폭 메모리(HBM) 생산 집중에 따른 가격 상승 기대 때문입니다.\\n\\n종합하면, 삼성전자는 하반기에도 파운드리와 고대역폭 메모리 부문을 중심으로 실적 개선과 주가 상승의 긍정적 흐름이 이어질 것으로 예상됩니다.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "general_llm()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-kr-qU3nSgPx-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
